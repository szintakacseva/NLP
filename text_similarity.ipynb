{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# References\n",
    "\n",
    "[DataCamp fuzzy-string tutorial](https://www.datacamp.com/tutorial/fuzzy-string-python)   \n",
    "[Text Similarity](https://livebook.manning.com/book/data-science-bookcamp/chapter-13/95)\n",
    "[Ultimate guide to text similarity](https://newscatcherapi.com/blog/ultimate-guide-to-text-similarity-with-python)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tags \n",
    "NLP, Levenstein distance, fuzzy matching, Jaccard Similarity, Tanimato Similarity ,Cosine Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Differences between Jaccard Similarity and Cosine Similarity:\n",
    "\n",
    "1. Jaccard similarity takes only unique set of words for each sentence / document while cosine similarity takes total length of the vectors. (these vectors could be made from bag of words term frequency or tf-idf)\n",
    "\n",
    "2. This means that if you repeat the word “friend” in Sentence 1 several times, cosine similarity changes but Jaccard similarity does not. For ex, if the word “friend” is repeated in the first sentence 50 times, cosine similarity drops to 0.4 but Jaccard similarity remains at 0.5.\n",
    "\n",
    "3. Jaccard similarity is good for cases where duplication does not matter, cosine similarity is good for cases where duplication matters while analyzing text similarity. For two product descriptions, it will be better to use Jaccard similarity as repetition of a word does not reduce their similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting help\n",
    "\n",
    "# help(plt.hist)\n",
    "# help(plt.legend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Levenshtein Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "\n",
    "The Levenshtein Distance\n",
    "The Levenshtein distance is a metric to measure how apart are two sequences of words. \n",
    "In other words, it measures the minimum number of edits that you need to do to change a one-word sequence into the other. \n",
    "These edits can be insertions, deletions or substitutions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From scratch\n",
    "\n",
    "import numpy as np\n",
    "def levenshtein_ratio_and_distance(s, t, ratio_calc = False):\n",
    "    \"\"\" levenshtein_ratio_and_distance:\n",
    "        Calculates levenshtein distance between two strings.\n",
    "        If ratio_calc = True, the function computes the\n",
    "        levenshtein distance ratio of similarity between two strings\n",
    "        For all i and j, distance[i,j] will contain the Levenshtein\n",
    "        distance between the first i characters of s and the\n",
    "        first j characters of t\n",
    "    \"\"\"\n",
    "    # Initialize matrix of zeros\n",
    "    rows = len(s)+1\n",
    "    cols = len(t)+1\n",
    "    distance = np.zeros((rows,cols),dtype = int)\n",
    "\n",
    "    # Populate matrix of zeros with the indeces of each character of both strings\n",
    "    for i in range(1, rows):\n",
    "        for k in range(1,cols):\n",
    "            distance[i][0] = i\n",
    "            distance[0][k] = k\n",
    "\n",
    "    # Iterate over the matrix to compute the cost of deletions,insertions and/or substitutions    \n",
    "    for col in range(1, cols):\n",
    "        for row in range(1, rows):\n",
    "            if s[row-1] == t[col-1]:\n",
    "                cost = 0 # If the characters are the same in the two strings in a given position [i,j] then the cost is 0\n",
    "            else:\n",
    "                # In order to align the results with those of the Python Levenshtein package, if we choose to calculate the ratio\n",
    "                # the cost of a substitution is 2. If we calculate just distance, then the cost of a substitution is 1.\n",
    "                if ratio_calc == True:\n",
    "                    cost = 2\n",
    "                else:\n",
    "                    cost = 1\n",
    "            distance[row][col] = min(distance[row-1][col] + 1,      # Cost of deletions\n",
    "                                 distance[row][col-1] + 1,          # Cost of insertions\n",
    "                                 distance[row-1][col-1] + cost)     # Cost of substitutions\n",
    "    if ratio_calc == True:\n",
    "        # Computation of the Levenshtein Distance Ratio\n",
    "        Ratio = ((len(s)+len(t)) - distance[row][col]) / (len(s)+len(t))\n",
    "        return Ratio\n",
    "    else:\n",
    "        # print(distance) # Uncomment if you want to see the matrix showing how the algorithm computes the cost of deletions,\n",
    "        # insertions and/or substitutions\n",
    "        # This is the minimum number of edits needed to convert string a to string b\n",
    "        return \"The strings are {} edits away\".format(distance[row][col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "# quick check\n",
    "\n",
    "Str1 = \"Apple Inc.\"\n",
    "Str2 = \"apple Inc\"\n",
    "\n",
    "result = Str1.lower() == Str2.lower()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The strings are 2 edits away\n",
      "0.8421052631578947\n"
     ]
    }
   ],
   "source": [
    "# Calculate levenshtein distance\n",
    "Str1 = \"Apple Inc.\"\n",
    "Str2 = \"apple Inc\"\n",
    "Distance = levenshtein_ratio_and_distance(Str1,Str2)\n",
    "print(Distance)\n",
    "Ratio = levenshtein_ratio_and_distance(Str1,Str2,ratio_calc = True)\n",
    "print(Ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fuzzy string comparison - fuzzywuzzy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95\n"
     ]
    }
   ],
   "source": [
    "# apply fuzzywuzzy package\n",
    "\n",
    "from fuzzywuzzy import fuzz\n",
    "Str1 = \"Apple Inc.\"\n",
    "Str2 = \"apple Inc\"\n",
    "Ratio = fuzz.ratio(Str1.lower(),Str2.lower())\n",
    "print(Ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "# Partial ratio - substrings\n",
    "\n",
    "Str1 = \"Los Angeles Lakers\"\n",
    "Str2 = \"Lakers\"\n",
    "Ratio = fuzz.ratio(Str1.lower(),Str2.lower())\n",
    "Partial_Ratio = fuzz.partial_ratio(Str1.lower(),Str2.lower())\n",
    "print(Ratio)\n",
    "print(Partial_Ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59\n",
      "74\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "# Token sort ratio - different order of substrings\n",
    "Str1 = \"united states v. nixon\"\n",
    "Str2 = \"Nixon v. United States\"\n",
    "Ratio = fuzz.ratio(Str1.lower(),Str2.lower())\n",
    "Partial_Ratio = fuzz.partial_ratio(Str1.lower(),Str2.lower())\n",
    "Token_Sort_Ratio = fuzz.token_sort_ratio(Str1,Str2)\n",
    "print(Ratio)\n",
    "print(Partial_Ratio)\n",
    "print(Token_Sort_Ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token set ratio - very different lenghts\n",
    "Str1 = \"The supreme court case of Nixon vs The United States\"\n",
    "Str2 = \"Nixon v. United States\"\n",
    "Ratio = fuzz.ratio(Str1.lower(),Str2.lower())\n",
    "Partial_Ratio = fuzz.partial_ratio(Str1.lower(),Str2.lower())\n",
    "Token_Sort_Ratio = fuzz.token_sort_ratio(Str1,Str2)\n",
    "Token_Set_Ratio = fuzz.token_set_ratio(Str1,Str2)\n",
    "print(Ratio)\n",
    "print(Partial_Ratio)\n",
    "print(Token_Sort_Ratio)\n",
    "print(Token_Set_Ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Apple Inc.', 100), ('apple incorporated', 90), ('apple park', 67), ('iphone', 30)]\n",
      "('Apple Inc.', 100)\n"
     ]
    }
   ],
   "source": [
    "#  process - allows to calculate the string with the highest similarity out of a vector of strings\n",
    "\n",
    "from fuzzywuzzy import process\n",
    "str2Match = \"apple inc\"\n",
    "strOptions = [\"Apple Inc.\",\"apple park\",\"apple incorporated\",\"iphone\"]\n",
    "Ratios = process.extract(str2Match,strOptions)\n",
    "print(Ratios)\n",
    "# You can also select the string with the highest matching percentage\n",
    "highest = process.extractOne(str2Match,strOptions)\n",
    "print(highest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jackard similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = 'She sells seashells by the seashore.'\n",
    "text2 = '\"Seashells! The seashells are on sale! By the seashore.\"'\n",
    "text3 = 'She sells 3 seashells to John, who lives by the lake.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words in text 1\n",
      "['She', 'sells', 'seashells', 'by', 'the', 'seashore.']\n",
      "\n",
      "Words in text 2\n",
      "['\"Seashells!', 'The', 'seashells', 'are', 'on', 'sale!', 'By', 'the', 'seashore.\"']\n",
      "\n",
      "Words in text 3\n",
      "['She', 'sells', '3', 'seashells', 'to', 'John,', 'who', 'lives', 'by', 'the', 'lake.']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Splitting texts into words\n",
    "words_lists = [text.split() for text in [text1, text2, text3]]\n",
    "\n",
    "words1, words2, words3 = words_lists\n",
    " \n",
    "for i, words in enumerate(words_lists, 1):\n",
    "    print(f\"Words in text {i}\")\n",
    "    print(f\"{words}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words in text 1\n",
      "['she', 'sells', 'seashells', 'by', 'the', 'seashore']\n",
      "\n",
      "Words in text 2\n",
      "['seashells', 'the', 'seashells', 'are', 'on', 'sale', 'by', 'the', 'seashore']\n",
      "\n",
      "Words in text 3\n",
      "['she', 'sells', '3', 'seashells', 'to', 'john', 'who', 'lives', 'by', 'the', 'lake']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Removing case sensitivity and punctuation\n",
    "\n",
    "def simplify_text(text):\n",
    "    for punctuation in ['.', ',', '!', '?', '\"']:\n",
    "        text = text.replace(punctuation, '')\n",
    " \n",
    "    return text.lower()\n",
    " \n",
    "for i, words in enumerate(words_lists, 1):\n",
    "    for j, word in enumerate(words):\n",
    "        words[j] = simplify_text(word)\n",
    " \n",
    "    print(f\"Words in text {i}\")\n",
    "    print(f\"{words}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Words in text 1\n",
      "{'sells', 'by', 'she', 'seashells', 'the', 'seashore'}\n",
      "\n",
      "Unique Words in text 2\n",
      "{'by', 'seashells', 'sale', 'are', 'on', 'the', 'seashore'}\n",
      "\n",
      "Unique Words in text 3\n",
      "{'john', 'sells', 'by', 'the', 'she', 'lake', 'to', '3', 'who', 'lives', 'seashells'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Converting word lists to sets\n",
    "\n",
    "words_sets = [set(words) for words in words_lists]\n",
    "for i, unique_words in enumerate(words_sets, 1):\n",
    "    print(f\"Unique Words in text {i}\")\n",
    "    print(f\"{unique_words}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texts 1 and 2 share these 4 words:\n",
      "{'by', 'the', 'seashells', 'seashore'}\n",
      "\n",
      "Texts 1 and 3 share these 5 words:\n",
      "{'sells', 'by', 'she', 'seashells', 'the'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extracting overlapping words between two texts\n",
    "\n",
    "words_set1 = words_sets[0]\n",
    "for i, words_set in enumerate(words_sets[1:], 2):\n",
    "    shared_words = words_set1 & words_set\n",
    "    print(f\"Texts 1 and {i} share these {len(shared_words)} words:\")\n",
    "    print(f\"{shared_words}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texts 1 and 2 don't share these 5 words:\n",
      "{'she', 'sells', 'sale', 'are', 'on'}\n",
      "\n",
      "Texts 1 and 3 don't share these 7 words:\n",
      "{'john', 'lake', 'to', '3', 'who', 'lives', 'seashore'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extracting diverging words between two texts\n",
    "\n",
    "for i, words_set in enumerate(words_sets[1:], 2):\n",
    "    diverging_words = words_set1 ^ words_set\n",
    "    print(f\"Texts 1 and {i} don't share these {len(diverging_words)} words:\")\n",
    "    print(f\"{diverging_words}\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Together, texts 1 and 2 contain 9 unique words. These words are:\n",
      " {'by', 'sells', 'she', 'seashells', 'sale', 'are', 'on', 'the', 'seashore'}\n",
      "\n",
      "Together, texts 1 and 3 contain 12 unique words. These words are:\n",
      " {'john', 'by', 'she', 'to', '3', 'seashells', 'seashore', 'lake', 'sells', 'who', 'lives', 'the'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extracting the union of words between two texts\n",
    "    \n",
    "for i, words_set in enumerate(words_sets[1:], 2):\n",
    "    total_words = words_set1 | words_set\n",
    "    print(f\"Together, texts 1 and {i} contain {len(total_words)} \"\n",
    "          f\"unique words. These words are:\\n {total_words}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Together, texts 1 and 2 contain 9 unique words. \n",
      "44.44% of these words are shared. \n",
      "55.56% of these words diverge.\n",
      "\n",
      "Together, texts 1 and 3 contain 12 unique words. \n",
      "41.67% of these words are shared. \n",
      "58.33% of these words diverge.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extracting the percentage of shared words between two texts\n",
    "\n",
    "for i, words_set in enumerate(words_sets[1:], 2):\n",
    "    shared_words = words_set1 & words_set\n",
    "    diverging_words = words_set1 ^ words_set\n",
    "    total_words = words_set1 | words_set\n",
    "    assert len(total_words) == len(shared_words) + len(diverging_words)\n",
    "    percent_shared = 100 * len(shared_words) / len(total_words)\n",
    "    percent_diverging = 100 * len(diverging_words) / len(total_words)\n",
    " \n",
    "    print(f\"Together, texts 1 and {i} contain {len(total_words)} \"\n",
    "          f\"unique words. \\n{percent_shared:.2f}% of these words are \"\n",
    "          f\"shared. \\n{percent_diverging:.2f}% of these words diverge.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Texts 1 and 3 share 41.67% of total words. The remaining 58.33% of words diverge. Meanwhile, texts 1 and 2 share 44.44% of total words. That percentage is higher, and thus we can infer that text1 is more similar to text2 than to text3.\n",
    "\n",
    "We’ve essentially developed a simple metric for assessing similarities between texts. The metric works as follows:\n",
    "\n",
    "Given two texts, extract a list of words from each text.\n",
    "Count the unique words that are shared between the texts.\n",
    "Divide the shared word count by the total unique words across both texts. Our output is a fraction of the total words shared between texts.\n",
    "This similarity metric is referred to as the Jaccard similarity, or the Jaccard index.\n",
    "\n",
    "The Jaccard similarity between texts 1 and 2 is illustrated in figure 13.2, where the texts are represented as two circles. The left circle corresponds to text 1, and the right circle corresponds to text 2. Each circle contains the words in its corresponding text. The two circles intersect, and their intersection contains all words that are shared between the texts. The Jaccard similarity equals the fraction of total words that are present in the intersection. Four of the nine words in the diagram appear in the intersection. Therefore, the Jaccard similarity is equal to 4 / 9.\n",
    "\n",
    "Figure 13.2 A visualized representation of the Jaccard similarity between two texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Jaccard similarity between 'She sells seashells by the seashore.' and '\"Seashells! The seashells are on sale! By the seashore.\"' equals 0.4444.\n",
      "\n",
      "The Jaccard similarity between 'She sells seashells by the seashore.' and 'She sells 3 seashells to John, who lives by the lake.' equals 0.4167.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Computing the Jaccard similarity\n",
    "\n",
    "def jaccard_similarity(text_a, text_b):\n",
    "    word_set_a, word_set_b = [set(simplify_text(text).split())\n",
    "                              for text in [text_a, text_b]]\n",
    "    num_shared = len(word_set_a & word_set_b)\n",
    "    num_total = len(word_set_a | word_set_b)\n",
    "    return num_shared / num_total\n",
    " \n",
    "for text in [text2, text3]:\n",
    "    similarity = jaccard_similarity(text1, text)\n",
    "    print(f\"The Jaccard similarity between '{text1}' and '{text}' \"\n",
    "          f\"equals {similarity:.4f}.\" \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Efficiently computing the Jaccard similarity\n",
    "\n",
    "def jaccard_similarity_efficient(text_a, text_b):\n",
    "    word_set_a, word_set_b = [set(simplify_text(text).split())\n",
    "                              for text in [text_a, text_b]]\n",
    "    num_shared = len(word_set_a & word_set_b)\n",
    "    num_total = len(word_set_a) + len(word_set_b) - num_shared\n",
    "    return num_shared / num_total\n",
    " \n",
    "for text in [text2, text3]:\n",
    "    similarity = jaccard_similarity_efficient(text1, text)\n",
    "    assert similarity == jaccard_similarity(text1, text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tanimato similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our vocabulary contains 15 words. This vocabulary is:\n",
      "{'john': 0, 'by': 1, 'she': 2, 'lives': 3, 'to': 4, '3': 5, 'seashells': 6, 'seashore': 7, 'lake': 8, 'sells': 9, 'sale': 10, 'are': 11, 'who': 12, 'on': 13, 'the': 14}\n"
     ]
    }
   ],
   "source": [
    "#  Assigning words to numbers in a vocabulary\n",
    "words_set1, words_set2, words_set3 = words_sets\n",
    "total_words = words_set1 | words_set2 | words_set3    \n",
    "vocabulary = {word : i for i, word in enumerate(total_words)}\n",
    "value_to_word = {value: word for word, value in vocabulary.items()}\n",
    "print(f\"Our vocabulary contains {len(vocabulary)} words. \"\n",
    "      f\"This vocabulary is:\\n{vocabulary}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Transforming words into binary vectors\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    " \n",
    "vectors = []\n",
    "for i, words_set in enumerate(words_sets, 1):\n",
    "    vector = np.array([0] * len(vocabulary))\n",
    "    for word in words_set:\n",
    "        vector[vocabulary[word]] = 1\n",
    "    vectors.append(vector)\n",
    " \n",
    "sns.heatmap(vectors, annot=True,  cmap='YlGnBu',\n",
    "            xticklabels=vocabulary.keys(),\n",
    "yticklabels=['Text 1', 'Text 2', 'Text 3'])\n",
    "plt.yticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'by' is present in both texts 1 and 2\n",
      "'seashells' is present in both texts 1 and 2\n",
      "'seashore' is present in both texts 1 and 2\n",
      "'the' is present in both texts 1 and 2\n"
     ]
    }
   ],
   "source": [
    "# Finding shared words using vector arithmetic\n",
    "vector1, vector2 = vectors[:2]\n",
    "for i in range(len(vocabulary)):\n",
    "    if vector1[i] * vector2[i]:\n",
    "        shared_word = value_to_word[i]\n",
    "        print(f\"'{shared_word}' is present in both texts 1 and 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting shared words using vector arithmetic\n",
    "\n",
    "shared_word_count = sum(vector1[i] * vector2[i]\n",
    "                        for i in range(len(vocabulary)))\n",
    "assert shared_word_count == len(words_set1 & words_set2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing a vector dot product using NumPy\n",
    "shared_word_count = sum(vector1[i] * vector2[i]\n",
    "                        for i in range(len(vocabulary)))\n",
    "assert shared_word_count == len(words_set1 & words_set2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting total words using vector arithmetic\n",
    "assert vector1.dot(vector2) == shared_word_count\n",
    "assert vector1 @ vector2 == shared_word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert vector1 @ vector1 == len(words_set1)\n",
    "assert vector2 @ vector2 == len(words_set2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tanimato similarity     \n",
    "We are able to compute both shared word count and total unique word count using vector dot products. Essentially, we can compute the Jaccard similarity using only vector operations.\n",
    "This vectorized implementation of Jaccard is called the Tanimoto similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing text similarity using vector arithmetic\n",
    "def tanimoto_similarity(vector_a, vector_b):\n",
    "    num_shared = vector_a @ vector_b\n",
    "    num_total = vector_a @ vector_a + vector_b @ vector_b - num_shared\n",
    "    return num_shared / num_total\n",
    " \n",
    "for i, text in enumerate([text2, text3], 1):\n",
    "    similarity = tanimoto_similarity(vector1, vectors[i])\n",
    "    assert similarity == jaccard_similarity(text1, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
