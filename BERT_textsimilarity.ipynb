{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ff06fdde-53e5-4389-815c-1cd0fd788bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "# settings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#print all rows of a df in ipython shell \n",
    "pd.set_option('display.max_rows', None)\n",
    "#print all columns of a df in ipython shell \n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.float_format', lambda x: '%.5f' % x)\n",
    "pd.set_option(\"display.precision\", 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba0636c-ac71-49c0-916b-86b40f5d0d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install pandas\n",
    "!conda install -c conda-forge scikit-learn\n",
    "!conda install -c conda-forge huggingface_hub\n",
    "!conda install -c huggingface transformers\n",
    "!conda install seaborn\n",
    "!conda install matplotlib\n",
    "!conda install pytorch torchvision torchaudio cpuonly -c pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba505d4-ab86-4f83-9cb9-1d2009435dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision torchaudio\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec02ddb5-411a-4bf1-917d-f7875df78918",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "test_torch = torch.rand(100,1,28,28)\n",
    "print(test_torch.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7b7d32d9-6136-4a7b-ba9c-d1a3ce87c1a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between text1 and text2: 0.7531288862228394\n",
      "Similarity between text1 and text3: 0.6926480531692505\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Example texts\n",
    "text1 = \"Updated the content\"\n",
    "text2 = \"Successfully updated\"\n",
    "text3 = \"Modified successfully\"\n",
    "\n",
    "# Tokenize the input texts\n",
    "tokens1 = tokenizer.encode_plus(text1, add_special_tokens=True, return_tensors='pt')\n",
    "tokens2 = tokenizer.encode_plus(text2, add_special_tokens=True, return_tensors='pt')\n",
    "tokens3 = tokenizer.encode_plus(text3, add_special_tokens=True, return_tensors='pt')\n",
    "\n",
    "# Get the BERT embeddings for the texts\n",
    "with torch.no_grad():\n",
    "    outputs1 = model(**tokens1)\n",
    "    outputs2 = model(**tokens2)\n",
    "    outputs3 = model(**tokens3)\n",
    "    embeddings1 = outputs1.last_hidden_state.mean(dim=1)  # Average pooling\n",
    "    embeddings2 = outputs2.last_hidden_state.mean(dim=1)\n",
    "    embeddings3 = outputs3.last_hidden_state.mean(dim=1)\n",
    "\n",
    "# Calculate cosine similarity between text pairs\n",
    "similarity1_2 = cosine_similarity(embeddings1, embeddings2).item()\n",
    "similarity1_3 = cosine_similarity(embeddings1, embeddings3).item()\n",
    "\n",
    "print(\"Similarity between text1 and text2:\", similarity1_2)\n",
    "print(\"Similarity between text1 and text3:\", similarity1_3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bf2002-e315-4e4c-8550-df2b715c65a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Miscellous"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0330ae13-9f8c-46a2-bfe4-1da71a3cc4e2",
   "metadata": {},
   "source": [
    "#  ChatGPT\n",
    "Q1. Recommend the best LLM for text similarity\n",
    "\n",
    "Answer.\n",
    "When it comes to text similarity, one of the most effective and widely used algorithms is called Latent Semantic Analysis (LSA) or Latent Semantic Indexing (LSI). LSA is a statistical model that analyzes the relationships between a set of documents and the terms they contain. It represents documents and terms as vectors in a high-dimensional space, where the distance between vectors indicates their similarity.\n",
    "However, it's important to note that LSA has its limitations, such as the inability to capture more complex semantic relationships or nuances in meaning. Therefore, newer algorithms have been developed that perform even better in measuring text similarity. One such algorithm is BERT (Bidirectional Encoder Representations from Transformers).\n",
    "BERT is a transformer-based deep learning model that has achieved state-of-the-art results in various natural language processing tasks, including text similarity. It utilizes a pre-trained language model that learns contextual representations of words, allowing it to understand the relationships between words and sentences.\n",
    "If you're looking for an LLM (Language Model) specifically designed for text similarity, BERT is a strong recommendation. You can use pre-trained BERT models, such as \"bert-base-uncased\" or \"bert-large-uncased,\" which have been trained on a large corpus of text data. These models can be fine-tuned on your specific task or dataset to further improve their performance on text similarity.\n",
    "In addition to BERT, other models based on transformers, such as GPT-3, RoBERTa, and XLNet, have also shown promising results in measuring text similarity and understanding semantic relationships between words and sentences. These models offer more advanced capabilities and can be fine-tuned or used as feature extractors for text similarity tasks.\n",
    "It's worth noting that while these models are powerful, they can be computationally expensive and require substantial computational resources. Therefore, it's essential to consider your specific requirements and available resources when choosing the best LLM for text similarity.\n",
    "\n",
    "Q2. give a python code example how to use BERT for text similarity\n",
    "                                                                                                                                                                                                                                                                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0013f5-f31b-428f-a59a-b820b942ebd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
